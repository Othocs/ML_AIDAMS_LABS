{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?export=view&id=1qJ8NqAZolTBQY7lN-deZ8xEsU3dlUiLz' width=200></center>\n",
        "<center><img src='https://upload.wikimedia.org/wikipedia/commons/a/a4/Logo-essec.jpg' width=200></center>\n",
        "\n",
        "\n",
        "<h6><center></center></h6>\n",
        "\n",
        "<h1>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "<center>Bachelor AIDAMS</center>\n",
        "<center>Getting started with the various tools</center>\n",
        "    <center> Lab 2 : Introduction to supervised learning </center>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "</h1>"
      ],
      "metadata": {
        "id": "Se3s9wSzqUaI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mJR_VVPqAWO"
      },
      "source": [
        "The purpose of this session is to familiarize yourself with supervised classification and its evaluation. For classification, we will use the naive Bayes classifier, which will be explained in detail in a future session.\n",
        "\n",
        "For this lab, we will use the IRIS dataset, which was already used in LAB 1. You can go back to the beginning of LAB 1 to familiarize yourself with this dataset.\n",
        "\n",
        "In **scikit-learn**, any classification method, represented here by the variable $\\textrm{clf}$, uses two essential functions:\n",
        " + The **clf.fit(data,target)** function, which learns a model from data and stores it in the variable  $\\textrm{clf}$.\n",
        " + The **clf.predict(newdata)** function, which returns an array that stores, for each new input data point, the class predicted by the learned model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcevDsg9qAWQ"
      },
      "source": [
        "## EXERCISE 1: An example of classification\n",
        "\n",
        "The Naive Bayes Multinomial algorithm is a classification model based on Bayes' rule, with an assumption of attribute independence, where data is described by several attributes with discrete values. It is available in **scikit-learn** within the **naive_bayes** module, the documentation for which is available [here](http://scikit-learn.org/stable/modules/naive_bayes.html).\n",
        "\n",
        "Understanding the example below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYwAHPa7qAWQ"
      },
      "outputs": [],
      "source": [
        "import pylab as pl\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "from sklearn import naive_bayes\n",
        "\n",
        "nb= naive_bayes.MultinomialNB(fit_prior=True)\n",
        "irisData=datasets.load_iris()\n",
        "nb.fit(irisData.data[:-1],irisData.target[:-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flFDmhx_qAWR"
      },
      "outputs": [],
      "source": [
        "iris31=nb.predict(irisData.data[31].reshape(1, -1))\n",
        "print(iris31)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZAvrNTZqAWR"
      },
      "outputs": [],
      "source": [
        "irislast=nb.predict(irisData.data[-1].reshape(1, -1))\n",
        "print(irislast)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWPtMFjaqAWR"
      },
      "outputs": [],
      "source": [
        "p=nb.predict(irisData.data[:])\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wASK5GGaqAWS"
      },
      "source": [
        "What does the program below do? How do you interpret the results? What do you think of the training and test sets?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2tIpxLyqAWS"
      },
      "outputs": [],
      "source": [
        "nb= naive_bayes.MultinomialNB(fit_prior=True)\n",
        "irisData=datasets.load_iris()\n",
        "nb.fit(irisData.data[:99],irisData.target[:99])\n",
        "p=nb.predict(irisData.data[100:149])\n",
        "print(p)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Complete with your thoughts on the program\n",
        " Write here"
      ],
      "metadata": {
        "id": "AuqdyFF2rA9P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBPQq2RuqAWS"
      },
      "source": [
        "# Evaluating the performance of a classifier\n",
        "\n",
        "We will now focus on evaluating the performance of our classifier. For each learning process, we will therefore calculate:\n",
        "+  The learning error\n",
        "+  The estimate of the actual error by dividing the learning sample into two parts.\n",
        "+  The estimate of the actual error by **cross-validation**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIllKcgTqAWS"
      },
      "source": [
        "## Performance on the training set: apparent error\n",
        "\n",
        "One method for evaluating the performance of a classifier on the data set used to build it is to run the prediction on the same data used for training and compare the prediction results with the actual values provided by the **target** field.  Several approaches can be used for this.\n",
        "\n",
        "+ One approach is to count the number of differences between the table containing the predictions and the table containing the ground truth. The apparent error is then the number of differences in the number of data points in the sample used. Write the code to implement this approach.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSly_EzdqAWS"
      },
      "outputs": [],
      "source": [
        "def errorApp_method1(Y,Ypred):\n",
        "  # TO DO\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZocyQSsqAWT"
      },
      "source": [
        " + A second approach consists of using the **clf.score(X,Y)** method, which calculates the correct classification rate of the model learned by the $\\textrm{clf}$ method on a data set stored in $X$ and whose classes are stored in $Y$. The correct classification rate is $1 - \\textrm{clf.score}(X,Y)$. Test this approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxmbRahIqAWT"
      },
      "outputs": [],
      "source": [
        "def errorApp_method2():\n",
        "  # TO DO\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS3PgcJ_qAWT"
      },
      "source": [
        "On which data is there a prediction error?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9U4TnxUqAWT"
      },
      "source": [
        "## Generalization performance: true error\n",
        "\n",
        "### Estimating true error by dividing the training sample\n",
        "\n",
        "The principle consists of assuming the existence of a distribution $P(X,Y)$ on our sample (even if it is not known) and separating the sample $S$ into two parts: one part $S_1$ to learn the model and another part $S_2$ to test the learned model. We try to have $$|S_1| = 2 |S_2| = \\frac{2}{3} |S|$$\n",
        "$S_1$ and $S_2$ must be disjoint.\n",
        "\n",
        "Since we know the labels of the examples in $S_2$, we can compare the predictions of the model learned on $S_1$ to the actual targets in $S_2$.\n",
        "\n",
        " + Create a function **split(S)** to split a sample $S$ of pairs $(X,Y)$ assumed to be uniformly distributed into two parts, according to the principle mentioned above. The function will therefore return a table with 4 elements: $[\\textrm{dataS1,targetS1,dataS2,targetS2}]$.\n",
        "\n",
        "You can use the functions in Python's **random** package for this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-9owdtcqAWT"
      },
      "outputs": [],
      "source": [
        "from random import *\n",
        "\n",
        "def split(S):\n",
        "    # TO COMPLETE\n",
        "\n",
        "\n",
        "[dataS1,targetS1,dataS2,targetS2] = split(irisData)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhyhwjjwqAWT"
      },
      "source": [
        "Create a function **test(S,clf)** that splits $S$ into two parts, then trains a model on the first part using the **clf** estimator and tests this model on the other part, counting the errors made on the sample. Test this function on the IRIS dataset. What do you notice?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCE-FgzzqAWT"
      },
      "outputs": [],
      "source": [
        "def test(S,clf):\n",
        "    # TO COMPLETE\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_OwGJlRqAWU"
      },
      "source": [
        "Perform the **test** function test $t$ times on the same data set and with the same classification algorithm. Average the estimated errors. What average error do you obtain for $t=10,50,100,200,500,1000$? For each $t$, repeat the experiment 20 times. Is the average error stable or not? Interpret this result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4tX1AlKqAWU"
      },
      "outputs": [],
      "source": [
        "# TO COMPLETE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH2_svi_qAWU"
      },
      "source": [
        "In **scikit-learn**, the **train_test_split** function from the **cross_validation** package allows you to split a sample into two disjoint parts. It takes a sample of data as input and separates it according to a proportion passed as a parameter. The documentation is available [here](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation). Test this function for several values of **test_size** to estimate the actual error of the algorithm on the IRIS data. What do you get?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0Wm6zAeqAWU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx3fXXYAqAWU"
      },
      "source": [
        "### Estimating the actual error using cross-validation\n",
        "\n",
        "As we saw in class, estimating the actual error of a classifier using cross-validation on $n$ parts (*folds*) is a very common approach. Given a sample $S$ and a learning algorithm $A$, the principle is as follows:\n",
        " + Separate $S$ into $n$ disjoint parts of similar sizes $S_i, i=1..n$.\n",
        " + For $i$ ranging from $1$ to $n$, train $A$ on all $S_j$ except $S_i$ to obtain the classifier $h_i$. Test $h_i$ on $S_i$. This gives $E_i$, the error made by $h_i$ on the part $E_i$.\n",
        " + Calculate\n",
        "$$E = \\sum_i\\frac{E_i}{n}$$\n",
        "\n",
        "\n",
        "Test the **cross_val_score** function of the **cross_validation** module to estimate the actual error by cross-validation $2,3,5,8$ and $10$ folds.\n",
        "Take this opportunity to take a close look at the **cross_validation** package in scikit-learn and test the different ways of partitioning the training set. You should also take the time to look at the different [metrics](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter). In particular, test the different metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Paw0cJtlqAWU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqbpcEzWqAWU"
      },
      "source": [
        "Change classification metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPCHp8yEqAWU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import *\n",
        "\n",
        "#TO COMPLETE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCEl7GEPqAWU"
      },
      "source": [
        "### SUMMARY\n",
        "Create a table summarizing the various errors obtained on the IRIS dataset and comment on it.\n",
        "\n",
        "##### to complete here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtKD82ogqAWV"
      },
      "source": [
        "## EXERCISE 2: Recognizing a dog from a cat (bonus)\n",
        "\n",
        "\n",
        "The goal of this exercise is to write a program capable of [**understanding an image**](https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures).\n",
        "\n",
        "In particular, we will focus on recognizing cats and dogs in images. Your program will take an image as input and assign it to either the **dog** or **cat** class. We will work with a sample from the Kaggle challenge [**Dogs vs Cats**](https://www.kaggle.com/c/dogs-vs-cats). This challenge was launched in 2013 and won by [Pierre Sermanet](https://sermanet.github.io/), currently a researcher at Google DeepMind. He used the deep learning library [Overfeat](http://cilvr.nyu.edu/doku.php?id=software:overfeat:start#overfeatobject_recognizer_feature_extractor) library, the result of his doctoral work at New York University under the supervision of [Yann Le Cun](http://yann.lecun.com/), Director of Research at Meta. He achieved a classification error rate of $1.09%$. We will not be using deep learning in this exercise, so we will be far from this score. The goal here is rather to set up a classic supervised learning pipeline.\n",
        "\n",
        "### Classification pipeline\n",
        "\n",
        "The image classification problem we are dealing with here consists of assigning one of two labels to the input image: [dog, cat].\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1iWbe4kHpEmk1X7jGUDjAUUFrKEob_BlU' width=200></center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This task is very easy for a human, but for a machine, the following challenges must be taken into account:\n",
        "+ Change of perspective.\n",
        "+ Change of scale.\n",
        "+ Change in lighting conditions.\n",
        "+ Deformation.\n",
        "+ Occlusion.\n",
        "+ Diversity of appearance for the same class.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1fC9HeTZxhxnnA1NK6PVCbXZRk8bvJSa5' width=200></center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1IByXZZx-hLtXHXHS6AyuB7HzKFKr162G' width=200></center>\n",
        "\n",
        "Source: Images from the CS231n course at Stanford (Convolutional Neural Networks for Visual Recognition)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqGyWDZQqAWV"
      },
      "source": [
        "This involves setting up the classic learning chain:\n",
        "1. Collect and prepare the data and its field truth.\n",
        "2. Train a classifier.\n",
        "3. Evaluate the classifier on new images.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1aGYb-4Zxu0g38HeK8bFYUUXHeHfeOhBO' width=200></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVD2gLkFqAWV"
      },
      "source": [
        "### Getting started with the data\n",
        "\n",
        "First, you need to download the dataset [DataDogsCatsChallenge.zip](http://files.fast.ai/data/dogscats.zip). It contains 25,000 images of cats and dogs in the **training set** and 12,500 in the **test set**.  \n",
        "\n",
        "You should also build a **validation set**.\n",
        "\n",
        "Since this dataset is relatively large, it is good practice to first develop your program on a small sample and then apply it to the entire dataset. This sample is available in the [Data](./Data/) directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuI6SzI3qAWV"
      },
      "source": [
        "## Image representations\n",
        "\n",
        "In this exercise, we choose to represent our image using two different representations:\n",
        "+ The first representation is constructed from the raw data by resizing the image to a fixed size e ($32 \\times 32$ pixels) and concatenating the R, G, and B values of the different pixels into a single vector of numbers.  \n",
        "+ A second representation consists of constructing the color histogram of each image and using this histogram as a representation.\n",
        "\n",
        "Some useful functions:\n",
        "+ With Numpy:\n",
        "    + [Array flattening](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html)\n",
        "+ With Scikit-image:\n",
        "    + [Loading an image from a file](http://scikit-image.org/docs/dev/api/skimage.data.html#skimage.data.imread)\n",
        "    + [Image transformations](http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.resize)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyDo9SnMqAWV"
      },
      "outputs": [],
      "source": [
        "# TO DO : complete import modules\n",
        "def image_to_feature_vector(image, size=(32, 32)):\n",
        "    # resize the image to a fixed size, then flatten the image into a list of raw pixel intensities\n",
        "\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "\n",
        "def build_RGB_color_histogram_vector(image,bins=(8, 8, 8)):\n",
        "    # extract a 3D color histogram from the RGB color space using the supplied number of `bins` per channel and return it as a feature vector\n",
        "\n",
        "    # TO COMPLETE\n",
        "\n",
        "    return res\n",
        "\n",
        "image_to_feature_vector(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr7nW47vqAWV"
      },
      "source": [
        "#### Data preparation and representation extraction\n",
        "\n",
        "+ Create three lists to store the raw representation, the histogram representation, and the image labels.\n",
        "+ Fill these three lists using the previous functions (first on the sample, then on the complete dataset).\n",
        "\n",
        "\n",
        "The **paths.py** file available [here](https://github.com/jrosebr1/imutils/blob/master/imutils/paths.py) can be used for this step.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDQTkHNzqAWW"
      },
      "outputs": [],
      "source": [
        "from paths import *\n",
        "\n",
        "print(\"Describing images...\")\n",
        "imagePaths = list(list_images('./data/DogCatChallenge/sample'))\n",
        "\n",
        "# initialize the raw pixel intensities matrix, the features matrix and labels list\n",
        "rawImages_features = []\n",
        "histogram_features = []\n",
        "labels = []\n",
        "\n",
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ENP_Cp9qAWW"
      },
      "source": [
        "#### Separating the dataset into training and validation sets\n",
        "\n",
        "To do this, we will use the [Cross validation](http://scikit-learn.org/stable/modules/cross_validation.html) module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe5OuQ7tqAWW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TO COMPLETE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-KBoaEmqAWW"
      },
      "source": [
        "#### Classification using the k-nearest neighbors classifier.\n",
        "\n",
        "\n",
        "Here we will use a very simple classifier: the k-nearest neighbors classifier, which will be discussed in class. Here we will use the Euclidean distance as the distance function:\n",
        "\n",
        "$d(p,q)=\\sqrt{\\sum_{i=1}^{N}(q_i-p_i)^2}$\n",
        "\n",
        "Some useful functions:\n",
        "+ [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
        "+ [Example](http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py)\n",
        "+ [An introduction to machine learning with scikit-learn](http://scikit-learn.org/stable/tutorial/basic/tutorial.html#learning-and-predicting)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTB_UfDcqAWW"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# training and evaluation of a k-NN classifer on the raw pixel representation\n",
        "\n",
        "# TO COMPLETE\n",
        "\n",
        "print(\"Raw pixel representation accuracy:\")\n",
        "\n",
        "# training and evaluation of  a k-NN classifer on the color histogram representation\n",
        "\n",
        "# TO COMPLETE\n",
        "\n",
        "print(\"Color histogram representation accuracy:\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyYIWZ88qAWW"
      },
      "source": [
        "Validation sets for hyperparameter tuning.\n",
        "\n",
        "The k-nearest neighbors classifier has k as its hyperparameter. What value did you choose? Another hyperparameter is the choice of distance.\n",
        "\n",
        "To choose and select the value of these hyperparameters, the principle is to use a **validation set** in addition to the training set and the test set.\n",
        "\n",
        "In scikit-learn, two approaches are available for choosing these hyperparameters: **Grid search** and **Randomized Search**:\n",
        " + [Tuning the hyper-parameters of an estimator](http://scikit-learn.org/stable/modules/grid_search.html)\n",
        "\n",
        "\n",
        "We will apply these two approaches:\n",
        " 1. First, define a dictionary of parameters with two keys:\n",
        "     + n_neighbors, the number of neighbors $k$ considered in the k-NN algorithm in the range $[1, 29]$\n",
        "     + metric, the distance used in the algorithm, which can be either Euclidean distance or Manhattan distance.\n",
        " 2. Then apply the **Grid search** and **Randomized Search** approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q8Dh53mqAWX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import time\n",
        "\n",
        "print(\"Building training/testing split...\")\n",
        "\n",
        "# TO COMPLETE\n",
        "\n",
        "params = {\"n_neighbors\": np.arange(1, 29, 1),\"metric\": [\"euclidean\", \"cityblock\"]}\n",
        "\n",
        "# tune the hyperparameters via a cross-validated grid search of the KNN classifier on raw features\n",
        "\n",
        "print(\"Tuning hyperparameters via grid search on raw features\")\n",
        "# TO COMPLETE\n",
        "start = time.time()\n",
        "# TO COMPLETE\n",
        "\n",
        "# evaluate the best grid searched model on the testing data\n",
        "\n",
        "# TO COMPLETE : print the time and the obtained best parameters\n",
        "\n",
        "# tune the hyperparameters via a cross-validated randomized search of the KNN classifier on raw features\n",
        "\n",
        "print(\"Tuning hyperparameters via randomized search on raw features\")\n",
        "# TO COMPLETE\n",
        "start = time.time()\n",
        "# TO COMPLETE\n",
        "\n",
        "# evaluate the best grid searched model on the testing data\n",
        "\n",
        "# TO COMPLETE : print the time and the obtained best parameters\n",
        "\n",
        "# tune the hyperparameters via a cross-validated grid search of the KNN classifier on histo features\n",
        "\n",
        "print(\"Tuning hyperparameters via grid search on histo features\")\n",
        "# TO COMPLETE\n",
        "start = time.time()\n",
        "# TO COMPLETE\n",
        "\n",
        "# evaluate the best grid searched model on the testing data\n",
        "\n",
        "# TO COMPLETE : print the time and the obtained best parameters\n",
        "\n",
        "# tune the hyperparameters via a cross-validated randomized search of the KNN classifier on raw features\n",
        "\n",
        "print(\"Tuning hyperparameters via randomized search on histo features\")\n",
        "# TO COMPLETE\n",
        "start = time.time()\n",
        "# TO COMPLETE\n",
        "\n",
        "# evaluate the best grid searched model on the testing data\n",
        "\n",
        "# TO COMPLETE : print the time and the obtained best parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9gqpWQjqAWX"
      },
      "source": [
        "Cross-validation\n",
        "\n",
        "Apply k-fold cross-validation (note that k is not the same as the k in the k-NN classifier). Write a 10-fold cross-validation for choosing the parameter k in the interval [1,29].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfphi8KRqAWX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "# creating odd list of K for KNN\n",
        "params = {\"n_neighbors\": np.arange(1, 29, 2)}\n",
        "\n",
        "# empty list that will hold cv scores\n",
        "cv_scores = []\n",
        "\n",
        "# perform 10-fold cross validation on the KNN classifier with raw features\n",
        "print(\" 10-fold cross validation on the KNN classifier with raw features\")\n",
        "\n",
        "# TO COMPLETE\n",
        "\n",
        "print(\"Plotting the misclassification error versus k\")\n",
        "\n",
        "# changing to misclassification error\n",
        "MSE = [1 - x for x in cv_scores]\n",
        "\n",
        "# determining best k\n",
        "optimal_k =0\n",
        "# TO MODIFY\n",
        "print (\"The optimal number of neighbors is \",  optimal_k)\n",
        "\n",
        "# plot misclassification error vs k\n",
        "plt.plot(params[\"n_neighbors\"], MSE)\n",
        "plt.xlabel('Number of Neighbors K')\n",
        "plt.ylabel('Misclassification Error')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# empty list that will hold cv scores\n",
        "cv_scores = []\n",
        "\n",
        "# perform 10-fold cross validation on the KNN classifier with raw features\n",
        "print(\" 10-fold cross validation on the KNN classifier with hist features\")\n",
        "\n",
        "# TO COMPLETE\n",
        "\n",
        "print(\"Plotting the misclassification error versus k\")\n",
        "\n",
        "# changing to misclassification error\n",
        "MSE = [1 - x for x in cv_scores]\n",
        "\n",
        "# determining best k\n",
        "optimal_k =0\n",
        "# TO MODIFY\n",
        "print (\"The optimal number of neighbors is \", optimal_k)\n",
        "\n",
        "# plot misclassification error vs k\n",
        "plt.plot(params[\"n_neighbors\"], MSE)\n",
        "plt.xlabel('Number of Neighbors K')\n",
        "plt.ylabel('Misclassification Error')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N833GyEpqAWX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}